{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Recognition with CNNs using Tensorflow/Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blah blah blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip -m install tensorflow\n",
    "pip -m install pandas\n",
    "pip -m install numpy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in file names from directories, convert images to numpy arrays of rgb values, save training and test arrays, label data and save labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 of 25000\n",
      "Processed 10000 of 25000\n",
      "Processed 20000 of 25000\n",
      "Processed 0 of 12500\n",
      "Processed 10000 of 12500\n",
      "Train shape: (25000, 3, 256, 256)\n",
      "Test shape: (12500, 3, 256, 256)\n",
      "saving images as numpy arrays...\n",
      "saving labels...\n"
     ]
    }
   ],
   "source": [
    "import os, cv2, random, h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import seaborn as sns\n",
    "\n",
    "#get file names for all train and test images\n",
    "os.chdir('/Users/stevenhurwitt/Documents/Python/convnet/')\n",
    "TRAIN_DIR = '/Users/stevenhurwitt/Documents/Python/convnet/train/'\n",
    "TEST_DIR = '/Users/stevenhurwitt/Documents/Python/convnet/test/'\n",
    "\n",
    "ROWS = 256\n",
    "COLS = 256\n",
    "CHANNELS = 3\n",
    "\n",
    "#train_images = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)] # use this for full dataset\n",
    "train_dogs =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'dog' in i]\n",
    "train_cats =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'cat' in i]\n",
    "\n",
    "test_images =  [TEST_DIR+i for i in os.listdir(TEST_DIR)]\n",
    "\n",
    "\n",
    "train_images = train_dogs + train_cats\n",
    "random.shuffle(train_images)\n",
    "test_images =  test_images\n",
    "\n",
    "#read the image into a matrix of rgb values\n",
    "def read_image(file_path):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_COLOR) #cv2.IMREAD_GRAYSCALE\n",
    "    b,g,r = cv2.split(img)\n",
    "    img2 = cv2.merge([r,g,b])\n",
    "    return cv2.resize(img2, (ROWS, COLS), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "#read in all images to data frame\n",
    "def prep_data(images):\n",
    "    count = len(images)\n",
    "    data = np.ndarray((count, CHANNELS, ROWS, COLS), dtype=np.uint8)\n",
    "\n",
    "    for i, image_file in enumerate(images):\n",
    "        image = read_image(image_file)\n",
    "        data[i] = image.T\n",
    "        if i%10000 == 0: print('Processed {} of {}'.format(i, count))\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "train = prep_data(train_images)\n",
    "test = prep_data(test_images)\n",
    "\n",
    "print(\"Train shape: {}\".format(train.shape))\n",
    "print(\"Test shape: {}\".format(test.shape))\n",
    "\n",
    "print(\"saving images as numpy arrays...\")\n",
    "np.savez(\"train\", train)\n",
    "np.savez(\"test\", test)\n",
    "\n",
    "labels = []\n",
    "for i in train_images:\n",
    "    if 'dog' in i:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "\n",
    "print(\"saving labels...\")\n",
    "with open('labels.data', 'wb') as filehandle:  \n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(labels, filehandle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data, define model, run model, save predictions & model history, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dropout, Flatten, Conv2D, MaxPooling2D, Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "\n",
    "train_npz = np.load(\"train.npz\")\n",
    "test_npz = np.load(\"test.npz\")\n",
    "\n",
    "train = train_npz['arr_0']\n",
    "test = test_npz['arr_0']\n",
    "\n",
    "with open('labels.data', 'rb') as filehandle:  \n",
    "    # read the data as binary data stream\n",
    "    labels = pickle.load(filehandle)\n",
    "\n",
    "optimizer = RMSprop(lr=1e-4)\n",
    "objective = 'binary_crossentropy'\n",
    "\n",
    "def catdog():\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, 3, padding='same', input_shape=train.shape[1:], activation='relu'))\n",
    "    model.add(Conv2D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\"))\n",
    "    print(\"First layer...\")\n",
    "    model.add(Conv2D(64, 3, padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\"))\n",
    "    print(\"Second layer...\")\n",
    "    model.add(Conv2D(128, 3, padding='same', activation='relu'))\n",
    "    model.add(Conv2D(128, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\"))\n",
    "    print(\"Third layer...\")\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\"))\n",
    "\n",
    "    print(\"Flattening, etc...\")\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    print(\"Compiling model...\")\n",
    "    model.compile(loss=objective, optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print(\"Creating model:\")\n",
    "model = catdog()\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "## Callback for loss logging per epoch\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')        \n",
    "       \n",
    "\n",
    "def run_catdog():\n",
    "    \n",
    "    history = LossHistory()\n",
    "    print(\"running model...\")\n",
    "    model.fit(train, labels, batch_size=batch_size, epochs=epochs,\n",
    "              validation_split=0.25, verbose=2, shuffle=True, callbacks=[history, early_stopping])\n",
    "\n",
    "    print(\"saving model...\")\n",
    "    model_json = model.to_json()\n",
    "    with open(\"catdog.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(\"catdog.h5\")\n",
    "    \n",
    "    print(\"making predictions on test set...\")\n",
    "    predictions = model.predict(test, verbose=0)\n",
    "    return predictions, history\n",
    "\n",
    "predictions, history = run_catdog()\n",
    "\n",
    "#save model, predictions & history\n",
    "model.save('catdog.hdf5')\n",
    "\n",
    "with open('preds.data', 'wb') as filehandle:  \n",
    "    pickle.dump(predictions, filehandle)\n",
    "\n",
    "with open('history.data', 'wb') as filehandle:  \n",
    "    pickle.dump(history, filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data, labels, model, predictions & history. plot loss over epochs on training & validation data, show sample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load training/test data\n",
    "print(\"loading data...\")\n",
    "\n",
    "train_npz = np.load(\"train.npz\")\n",
    "test_npz = np.load(\"test.npz\")\n",
    "\n",
    "train = train_npz['arr_0']\n",
    "test = test_npz['arr_0']\n",
    "\n",
    "#load labels\n",
    "print(\"loading labels...\")\n",
    "\n",
    "with open('labels.data', 'rb') as filehandle:  \n",
    "    # read the data as binary data stream\n",
    "    labels = pickle.load(filehandle)\n",
    "\n",
    "\n",
    "#load model\n",
    "print(\"loading model...\")\n",
    "json_file = open('catdog.json', 'r')\n",
    "\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "loaded_model.load_weights(\"catdog.h5\")\n",
    "print(\"loaded model from disk\")\n",
    "\n",
    "loaded_model=load_model('catdog.hdf5')\n",
    "\n",
    "print(\"loading predictions\")\n",
    "\n",
    "with open('preds.data', 'rb') as filehandle:  \n",
    "    predictions = pickle.load(filehandle)\n",
    "\n",
    "with open('history.data', 'rb') as filehandle:  \n",
    "    history = pickle.load(filehandle)\n",
    "\n",
    "\n",
    "loss = history.losses\n",
    "val_loss = history.val_losses\n",
    "\n",
    "json_file = open('catdog.json', 'r')\n",
    "\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "loaded_model.load_weights(\"catdog.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "loaded_model=load_model('catdog.hdf5')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('VGG-16 Loss Trend')\n",
    "plt.plot(loss, 'blue', label='Training Loss')\n",
    "plt.plot(val_loss, 'green', label='Validation Loss')\n",
    "plt.xticks(range(0,epochs)[0::2])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "   \n",
    "#show sample of predictions\n",
    "for i in range(0,10):\n",
    "    if predictions[i, 0] >= 0.5: \n",
    "        print('I am {:.2%} sure this is a Dog'.format(predictions[i][0]))\n",
    "    else: \n",
    "        print('I am {:.2%} sure this is a Cat'.format(1-predictions[i][0]))\n",
    "        \n",
    "    plt.imshow(test[i].T)\n",
    "    plt.pause(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
