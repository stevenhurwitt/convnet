{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Recognition with CNNs using Tensorflow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/media/steven/big_boi/convnet-data')\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported modules...\n",
      "using tensorflow version: 1.4.0.\n"
     ]
    }
   ],
   "source": [
    "import os, cv2, random, h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import seaborn as sns\n",
    "print('imported modules...')\n",
    "import tensorflow as tf\n",
    "print('using tensorflow version: {}.'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set directories...\n"
     ]
    }
   ],
   "source": [
    "#get file names for all train and test images\n",
    "os.chdir('/media/steven/big_boi/convnet-data/convnet')\n",
    "TRAIN_DIR = '/media/steven/big_boi/convnet-data/train'\n",
    "TEST_DIR = '/media/steven/big_boi/convnet-data/test'\n",
    "\n",
    "ROWS = 256\n",
    "COLS = 256\n",
    "CHANNELS = 3\n",
    "\n",
    "#train_images = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)] # use this for full dataset\n",
    "train_dogs =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'dog' in i]\n",
    "train_cats =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'cat' in i]\n",
    "\n",
    "test_images =  [TEST_DIR+i for i in os.listdir(TEST_DIR)]\n",
    "\n",
    "\n",
    "train_images = train_dogs + train_cats\n",
    "random.shuffle(train_images)\n",
    "test_images =  test_images\n",
    "print('set directories...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the image into a matrix of rgb values\n",
    "def read_image(file_path):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_COLOR) #cv2.IMREAD_GRAYSCALE\n",
    "    b,g,r = cv2.split(img)\n",
    "    img2 = cv2.merge([r,g,b])\n",
    "    return cv2.resize(img2, (ROWS, COLS), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "#read in all images to data frame\n",
    "def prep_data(images):\n",
    "    count = len(images)\n",
    "    data = np.ndarray((count, CHANNELS, ROWS, COLS), dtype=np.uint8)\n",
    "\n",
    "    for i, image_file in enumerate(images):\n",
    "        if (i > 0 and i % 10000 == 0): \n",
    "            print('Processed {} of {}'.format(i, count))\n",
    "        \n",
    "        try:\n",
    "            image = read_image(image_file)\n",
    "            data[i] = image.T\n",
    "            if i%10000 == 0: print('Processed {} of {}'.format(i, count))\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "Processed 10000 of 50000\n",
      "Processed 20000 of 50000\n",
      "Processed 30000 of 50000\n",
      "Processed 40000 of 50000\n",
      "Processed 10000 of 25000\n",
      "Processed 20000 of 25000\n",
      "Train shape: (50000, 3, 256, 256)\n",
      "Test shape: (25000, 3, 256, 256)\n",
      "saving images as numpy arrays...\n",
      "saving labels...\n"
     ]
    }
   ],
   "source": [
    "print('reading data...')\n",
    "train = prep_data(train_images)\n",
    "test = prep_data(test_images)\n",
    "\n",
    "print(\"Train shape: {}\".format(train.shape))\n",
    "print(\"Test shape: {}\".format(test.shape))\n",
    "\n",
    "print(\"saving images as numpy arrays...\")\n",
    "np.savez(\"train\", train)\n",
    "np.savez(\"test\", test)\n",
    "\n",
    "labels = []\n",
    "for i in train_images:\n",
    "    if 'dog' in i:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "\n",
    "print(\"saving labels...\")\n",
    "with open('labels.data', 'wb') as filehandle:  \n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(labels, filehandle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data, define model, run model, save predictions & model history, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dropout, Flatten, Conv2D, MaxPooling2D, Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "\n",
    "def catdog():\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, 3, padding='same', input_shape=train.shape[1:], activation='relu'))\n",
    "    model.add(Conv2D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\"))\n",
    "    print(\"First layer...\")\n",
    "    model.add(Conv2D(64, 3, padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\"))\n",
    "    print(\"Second layer...\")\n",
    "    model.add(Conv2D(128, 3, padding='same', activation='relu'))\n",
    "    model.add(Conv2D(128, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\"))\n",
    "    print(\"Third layer...\")\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\"))\n",
    "\n",
    "    print(\"Flattening, etc...\")\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    print(\"Compiling model...\")\n",
    "    model.compile(loss=objective, optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "## Callback for loss logging per epoch\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        \n",
    "def run_catdog():\n",
    "    \n",
    "    history = LossHistory()\n",
    "    print(\"running model...\")\n",
    "    model.fit(train, labels, batch_size=batch_size, epochs=epochs,validation_split=0.25, verbose=2, shuffle=True, callbacks=[history, early_stopping])\n",
    "\n",
    "    print(\"saving model...\")\n",
    "    model_json = model.to_json()\n",
    "    with open(\"catdog.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(\"catdog.h5\")\n",
    "    \n",
    "    print(\"making predictions on test set...\")\n",
    "    predictions = model.predict(test, verbose=0)\n",
    "    return predictions, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train and test data...\n",
      "loading labels...\n",
      "Creating model:\n",
      "First layer...\n",
      "Second layer...\n",
      "Third layer...\n",
      "Flattening, etc...\n",
      "Compiling model...\n"
     ]
    }
   ],
   "source": [
    "print('loading train and test data...')\n",
    "train_npz = np.load(\"train.npz\")\n",
    "test_npz = np.load(\"test.npz\")\n",
    "\n",
    "train = train_npz['arr_0']\n",
    "test = test_npz['arr_0']\n",
    "\n",
    "print('loading labels...')\n",
    "with open('labels.data', 'rb') as filehandle:  \n",
    "    # read the data as binary data stream\n",
    "    labels = pickle.load(filehandle)\n",
    "labels = np.array(labels)\n",
    "\n",
    "optimizer = RMSprop(lr=1e-4)\n",
    "objective = 'binary_crossentropy'\n",
    "\n",
    "print(\"Creating model:\")\n",
    "model = catdog()\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model...\n",
      "Train on 37500 samples, validate on 12500 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "predictions, history = run_catdog()\n",
    "\n",
    "#save model, predictions & history\n",
    "model.save('catdog.hdf5')\n",
    "\n",
    "with open('preds.data', 'wb') as filehandle:  \n",
    "    pickle.dump(predictions, filehandle)\n",
    "\n",
    "with open('history.data', 'wb') as filehandle:  \n",
    "    pickle.dump(history, filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data, labels, model, predictions & history. plot loss over epochs on training & validation data, show sample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training/test data\n",
    "print(\"loading data...\")\n",
    "\n",
    "train_npz = np.load(\"train.npz\")\n",
    "test_npz = np.load(\"test.npz\")\n",
    "\n",
    "train = train_npz['arr_0']\n",
    "test = test_npz['arr_0']\n",
    "\n",
    "#load labels\n",
    "print(\"loading labels...\")\n",
    "\n",
    "with open('labels.data', 'rb') as filehandle:  \n",
    "    # read the data as binary data stream\n",
    "    labels = pickle.load(filehandle)\n",
    "\n",
    "\n",
    "#load model\n",
    "print(\"loading model...\")\n",
    "json_file = open('catdog.json', 'r')\n",
    "\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "loaded_model.load_weights(\"catdog.h5\")\n",
    "print(\"loaded model from disk\")\n",
    "\n",
    "loaded_model=load_model('catdog.hdf5')\n",
    "\n",
    "print(\"loading predictions\")\n",
    "\n",
    "with open('preds.data', 'rb') as filehandle:  \n",
    "    predictions = pickle.load(filehandle)\n",
    "\n",
    "with open('history.data', 'rb') as filehandle:  \n",
    "    history = pickle.load(filehandle)\n",
    "\n",
    "\n",
    "loss = history.losses\n",
    "val_loss = history.val_losses\n",
    "\n",
    "json_file = open('catdog.json', 'r')\n",
    "\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "loaded_model.load_weights(\"catdog.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "loaded_model=load_model('catdog.hdf5')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('VGG-16 Loss Trend')\n",
    "plt.plot(loss, 'blue', label='Training Loss')\n",
    "plt.plot(val_loss, 'green', label='Validation Loss')\n",
    "plt.xticks(range(0,epochs)[0::2])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "   \n",
    "#show sample of predictions\n",
    "for i in range(0,10):\n",
    "    if predictions[i, 0] >= 0.5: \n",
    "        print('I am {:.2%} sure this is a Dog'.format(predictions[i][0]))\n",
    "    else: \n",
    "        print('I am {:.2%} sure this is a Cat'.format(1-predictions[i][0]))\n",
    "        \n",
    "    plt.imshow(test[i].T)\n",
    "    plt.pause(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "convnet",
   "language": "python",
   "name": "convnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
